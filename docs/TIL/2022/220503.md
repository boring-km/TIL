# 2022년 5월 3일 TIL

- JavaScript iframe 관련
- 모바일 머신러닝 모델 사용 (Android, iOS)

### JavaScript iframe 관련
- html 안에 iframe 영역으로 설정한 뒤 iframe 안에 새로운 html 생성
- iframe 안에 어떤 유형의 데이터를 넣으면 그 유형에 맞는 View를 생성하도록 사용하는 구조에서 주로 사용하는 듯
  - 회사에서는 웹에서 문제를 풀 때 문제유형을 모듈화 해서 iframe 영역 안에 정해진 문제 유형을 그리는 구조를 사용했다.
- iframe 안에서도 iframe 밖에 있는 function을 parent 키워드를 이용해 호출할 수 있다.

### 모바일 머신러닝 모델 사용
- [tensorflow examples](https://www.tensorflow.org/lite/examples?hl=ko)

#### Android
- 참고 예제: [https://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/android](https://github.com/tensorflow/examples/tree/master/lite/examples/style_transfer/android)

```kotlin
class StyleTransferModelExecutor(
  context: Context,
  private var useGPU: Boolean = false
) {
  private var gpuDelegate: GpuDelegate? = null
  private var numberThreads = 4

  private val interpreterPredict: Interpreter
  private val interpreterTransform: Interpreter

  private var fullExecutionTime = 0L
  private var preProcessTime = 0L
  private var stylePredictTime = 0L
  private var styleTransferTime = 0L
  private var postProcessTime = 0L
  
  init {
    if (useGPU) {
      interpreterPredict = getInterpreter(context, STYLE_PREDICT_FLOAT16_MODEL, true)
      interpreterTransform = getInterpreter(context, STYLE_TRANSFER_FLOAT16_MODEL, true)
    } else {
      interpreterPredict = getInterpreter(context, STYLE_PREDICT_INT8_MODEL, false)
      interpreterTransform = getInterpreter(context, STYLE_TRANSFER_INT8_MODEL, false)
    }
  }

  // .tflite 파일로 된 모델을 불러온다.
  companion object {
    private const val TAG = "StyleTransferMExec"
    private const val STYLE_IMAGE_SIZE = 256
    private const val CONTENT_IMAGE_SIZE = 384
    private const val BOTTLENECK_SIZE = 100
    private const val STYLE_PREDICT_INT8_MODEL = "style_predict_quantized_256.tflite"
    private const val STYLE_TRANSFER_INT8_MODEL = "style_transfer_quantized_384.tflite"
    private const val STYLE_PREDICT_FLOAT16_MODEL = "style_predict_f16_256.tflite"
    private const val STYLE_TRANSFER_FLOAT16_MODEL = "style_transfer_f16_384.tflite"
  }

  fun execute(
    contentImagePath: String,
    styleImageName: String,
    context: Context
  ): ModelExecutionResult {
    try {
      Log.i(TAG, "running models")

      fullExecutionTime = SystemClock.uptimeMillis()
      preProcessTime = SystemClock.uptimeMillis()

      val contentImage = ImageUtils.decodeBitmap(File(contentImagePath))
      val contentArray =
        ImageUtils.bitmapToByteBuffer(contentImage, CONTENT_IMAGE_SIZE, CONTENT_IMAGE_SIZE)
      val styleBitmap =
        ImageUtils.loadBitmapFromResources(context, "thumbnails/$styleImageName")
      val input = ImageUtils.bitmapToByteBuffer(styleBitmap, STYLE_IMAGE_SIZE, STYLE_IMAGE_SIZE)

      val inputsForPredict = arrayOf<Any>(input)
      val outputsForPredict = HashMap<Int, Any>()
      val styleBottleneck = Array(1) { Array(1) { Array(1) { FloatArray(BOTTLENECK_SIZE) } } }
      outputsForPredict[0] = styleBottleneck
      preProcessTime = SystemClock.uptimeMillis() - preProcessTime

      stylePredictTime = SystemClock.uptimeMillis()
      // The results of this inference could be reused given the style does not change
      // That would be a good practice in case this was applied to a video stream.
      // 예측 모델 실행 - 1
      interpreterPredict.runForMultipleInputsOutputs(inputsForPredict, outputsForPredict)
      stylePredictTime = SystemClock.uptimeMillis() - stylePredictTime
      Log.d(TAG, "Style Predict Time to run: $stylePredictTime")

      val inputsForStyleTransfer = arrayOf(contentArray, styleBottleneck)
      val outputsForStyleTransfer = HashMap<Int, Any>()
      val outputImage =
        Array(1) { Array(CONTENT_IMAGE_SIZE) { Array(CONTENT_IMAGE_SIZE) { FloatArray(3) } } }
      outputsForStyleTransfer[0] = outputImage

      styleTransferTime = SystemClock.uptimeMillis()
      // 예측 모델 실행 - 2
      interpreterTransform.runForMultipleInputsOutputs(
        inputsForStyleTransfer,
        outputsForStyleTransfer
      )
      styleTransferTime = SystemClock.uptimeMillis() - styleTransferTime
      Log.d(TAG, "Style apply Time to run: $styleTransferTime")

      postProcessTime = SystemClock.uptimeMillis()
      val styledImage =
        ImageUtils.convertArrayToBitmap(outputImage, CONTENT_IMAGE_SIZE, CONTENT_IMAGE_SIZE)
      postProcessTime = SystemClock.uptimeMillis() - postProcessTime

      fullExecutionTime = SystemClock.uptimeMillis() - fullExecutionTime
      Log.d(TAG, "Time to run everything: $fullExecutionTime")

      return ModelExecutionResult(
        styledImage,
        preProcessTime,
        stylePredictTime,
        styleTransferTime,
        postProcessTime,
        fullExecutionTime,
        formatExecutionLog()
      )
    } catch (e: Exception) {
      val exceptionLog = "something went wrong: ${e.message}"
      Log.d(TAG, exceptionLog)

      val emptyBitmap =
        ImageUtils.createEmptyBitmap(
          CONTENT_IMAGE_SIZE,
          CONTENT_IMAGE_SIZE
        )
      return ModelExecutionResult(
        emptyBitmap, errorMessage = e.message!!
      )
    }
  }

  @Throws(IOException::class)
  private fun loadModelFile(context: Context, modelFile: String): MappedByteBuffer {
    val fileDescriptor = context.assets.openFd(modelFile)
    val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
    val fileChannel = inputStream.channel
    val startOffset = fileDescriptor.startOffset
    val declaredLength = fileDescriptor.declaredLength
    val retFile = fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    fileDescriptor.close()
    return retFile
  }

  @Throws(IOException::class)
  private fun getInterpreter(
    context: Context,
    modelName: String,
    useGpu: Boolean = false
  ): Interpreter {
    val tfliteOptions = Interpreter.Options()
    tfliteOptions.numThreads = numberThreads

    gpuDelegate = null
    if (useGpu) {
      gpuDelegate = GpuDelegate()
      tfliteOptions.addDelegate(gpuDelegate)
    }

    tfliteOptions.numThreads = numberThreads
    return Interpreter(loadModelFile(context, modelName), tfliteOptions)
  }

  private fun formatExecutionLog(): String {
    val sb = StringBuilder()
    sb.append("Input Image Size: $CONTENT_IMAGE_SIZE x $CONTENT_IMAGE_SIZE\n")
    sb.append("GPU enabled: $useGPU\n")
    sb.append("Number of threads: $numberThreads\n")
    sb.append("Pre-process execution time: $preProcessTime ms\n")
    sb.append("Predicting style execution time: $stylePredictTime ms\n")
    sb.append("Transferring style execution time: $styleTransferTime ms\n")
    sb.append("Post-process execution time: $postProcessTime ms\n")
    sb.append("Full execution time: $fullExecutionTime ms\n")
    return sb.toString()
  }

  fun close() {
    interpreterPredict.close()
    interpreterTransform.close()
    if (gpuDelegate != null) {
      gpuDelegate!!.close()
    }
  }
}

```

1. tensorflow 모델을 lite 버전으로 모델을 하나의 파일로 만든다.
2. 그 파일을 읽어서 Interpreter 라는 객체로 생성한다.
3. Interpreter 객체에서 runForMultipleInputsOutputs() 메서드를 실행한다.
4. 예측 결과를 outputImage에 저장해 사용할 수 있도록 변환한 후 리턴한다.

#### iOS
- 이 예제에서 확인해보기: [https://github.com/MijeongJeon/KoreanClassification_Keras_Coreml](https://github.com/MijeongJeon/KoreanClassification_Keras_Coreml)
